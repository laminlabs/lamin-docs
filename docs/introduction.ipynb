{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![stars](https://img.shields.io/github/stars/laminlabs/lamindb?logo=GitHub&color=yellow)](https://github.com/laminlabs/lamindb)\n",
    "[![pypi](https://img.shields.io/pypi/v/lamindb?color=blue&label=pypi%20package)](https://pypi.org/project/lamindb)\n",
    "[![cran](https://www.r-pkg.org/badges/version/laminr?color=green)](https://cran.r-project.org/package=laminr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "LaminDB is an open-source data framework for better computational biology.\n",
    "It lets you track data transformations, curate datasets, manage metadata, and query a built-in database for common biological data types.\n",
    "\n",
    ":::{dropdown} Why?\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/BunYmHkyFLITlM5MYQck.svg\" width=\"350px\" style=\"background: transparent\" align=\"right\">\n",
    "\n",
    "Reproducing analytical results or understanding how a dataset or model was created can be a pain.\n",
    "Leave alone training models on historical data, orthogonal assays, or datasets generated by other teams.\n",
    "\n",
    "Biological datasets are typically managed with versioned storage systems (file systems, object storage, git, dvc), {term}`UI`-focused community or SaaS platforms, structureless data lakes, rigid data warehouses (SQL, monolithic arrays), and data lakehouses for tabular data.\n",
    "\n",
    "LaminDB goes beyond these systems with a lakehouse that models biological datasets beyond tables with enough structure to enable queries and enough freedom to keep the pace of R&D high.\n",
    "\n",
    "For data structures like `DataFrame`, `AnnData`, `.zarr`, `.tiledbsoma`, etc., LaminDB tracks and provides the rich context that collaborative biological research requires:\n",
    "\n",
    "- data lineage: data sources and transformations; scientists and machine learning models\n",
    "- domain knowledge and experimental metadata: the features and labels derived from domain entities\n",
    "\n",
    "In this [blog post](https://lamin.ai/blog/problems), we discuss a breadth of data management problems of the field.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} LaminDB specs\n",
    "\n",
    "```{include} includes/features-lamindb.md\n",
    "```\n",
    ":::\n",
    "\n",
    "LaminHub is a data collaboration hub built on LaminDB similar to how GitHub is built on git.\n",
    "\n",
    ":::{dropdown} LaminHub overview\n",
    "\n",
    "```{include} includes/features-laminhub.md\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "Install the `lamindb` Python package.\n",
    "\n",
    "```shell\n",
    "pip install 'lamindb[jupyter,bionty]'  # support notebooks & biological ontologies\n",
    "```\n",
    "\n",
    "Connect to a LaminDB instance.\n",
    "\n",
    "```shell\n",
    "lamin connect account/instance  # <-- replace with your instance\n",
    "```\n",
    "\n",
    "Access an input dataset and save an output dataset.\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Python\n",
    "```python\n",
    "import lamindb as ln\n",
    "\t\n",
    "ln.track()  # track a run of your notebook or script\n",
    "artifact = ln.Artifact.get(\"3TNCsZZcnIBv2WGb0001\")  # get an artifact by uid\n",
    "filepath = artifact.cache()  # cache the artifact on disk\n",
    "\n",
    "# do your work\n",
    "\n",
    "ln.Artifact(\"./my_dataset.csv\", key=\"my_results/my_dataset.csv\").save()  # save a file\n",
    "ln.finish()  # mark the run as finished & save a report for the current notebook/script\n",
    "```\n",
    ":::\n",
    ":::{tab-item} R\n",
    "\n",
    "```R\n",
    "# laminr needs pip install 'lamindb'\n",
    "install.packages(\"laminr\", dependencies = TRUE)  # install the laminr package from CRAN\n",
    "library(laminr)\n",
    "\n",
    "db <- connect()  # connect to the instance you configured on the terminal\n",
    "db$track()  # track a run of your notebook or script\n",
    "artifact <- db$Artifact$get(\"3TNCsZZcnIBv2WGb0001\")  # get an artifact by uid\n",
    "filepath <- artifact$cache()  # cache the artifact on disk\n",
    "\n",
    "# do your work\n",
    "\n",
    "db$Artifact.from_path(\"./my_dataset.csv\", key=\"my_results/my_dataset.csv\").save()  # save a folder\n",
    "db$finish()  # mark the run finished\n",
    "```\n",
    "\n",
    "Depending on whether you ran RStudio's notebook mode, you may need to save an html export for `.qmd` or `.Rmd` file via the command-line.\n",
    "\n",
    "```shell\n",
    "lamin save my-analysis.Rmd\n",
    "```\n",
    "\n",
    "For more, see the [R docs](https://laminr.lamin.ai/).\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LaminDB instance is a database that manages metadata for datasets in different storage locations. Let's create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!lamin init --storage ./lamin-intro --modules bionty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a cloud storage location to `--storage` (S3, GCP, R2, HF, etc.) or a Postgres database connection string to `--db`. See {doc}`setup`.\n",
    "If you decide to connect your LaminDB instance to the hub, you will see data & metadata in a UI.\n",
    "\n",
    "```{dropdown} On the hub.\n",
    "\n",
    "<a href=\"https://lamin.ai/laminlabs/lamindata\">\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/YuefPQlAfeHcQvtq0000.png\" width=\"700px\">\n",
    "</a>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "A data transformation (a \"transform\") is a piece of code (script, notebook, pipeline, function) that can be applied to input data to produce output data.\n",
    "\n",
    "When you call {meth}`~lamindb.track` in a script or notebook, inputs, outputs, source code, run reports and environments start to get automatically tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "import pandas as pd\n",
    "\n",
    "ln.track()  # track the current notebook or script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Is this compliant with OpenLineage?\n",
    "\n",
    "Yes. What OpenLineage calls a \"job\", LaminDB calls a \"transform\". What OpenLineage calls a \"run\", LaminDB calls a \"run\".\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all your transforms and their runs in the {class}`~lamindb.Transform` and {class}`~lamindb.Run` registries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Transform.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Run.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An {class}`~lamindb.Artifact` stores a dataset or model as a file or folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# an example dataset\n",
    "df = ln.core.datasets.small_dataset1(otype=\"DataFrame\", with_typo=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# create & save an artifact from a DataFrame\n",
    "artifact = ln.Artifact.from_df(df, key=\"my_datasets/rnaseq1.parquet\").save()\n",
    "\n",
    "# describe the artifact\n",
    "artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy or download the artifact into a local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "artifact.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the artifact for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = artifact.open()  # returns pyarrow.Dataset\n",
    "dataset.head(2).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache & load the artifact into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "artifact.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View data lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact.view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} How do I create an artifact for a file or folder?\n",
    "\n",
    "Source path is local:\n",
    "\n",
    "```python\n",
    "ln.Artifact(\"./my_data.fcs\", key=\"my_data.fcs\")\n",
    "ln.Artifact(\"./my_images/\", key=\"my_images\")\n",
    "```\n",
    "<br>\n",
    "\n",
    "Upon `artifact.save()`, the source path will be copied or uploaded into your instance's current storage, visible & changeable via `ln.settings.storage`.\n",
    "\n",
    "If the source path is remote or already in a registered storage location, `artifact.save()` won't trigger a copy or upload but register the existing path.\n",
    "\n",
    "```python\n",
    "ln.Artifact(\"s3://my-bucket/my_data.fcs\")  # key is auto-populated from S3, you can optionally pass a description\n",
    "ln.Artifact(\"s3://my-bucket/my_images/\")  # key is auto-populated from S3, you can optionally pass a description\n",
    "```\n",
    "<br>\n",
    "You can also use other remote file systems supported by `fsspec`.\n",
    "\n",
    ":::\n",
    "\n",
    "```{dropdown} How does LaminDB compare to a AWS S3?\n",
    "\n",
    "LaminDB provides a database on top of AWS S3 (or GCP storage, file systems, etc.).\n",
    "\n",
    "Similar to organizing files with paths, you can organize artifacts using the `key` parameter of {class}`~lamindb.Artifact`.\n",
    "\n",
    "However, you'll see that you can more conveniently query data by entities you care about: people, code, experiments, genes, proteins, cell types, etc.\n",
    "\n",
    "```\n",
    "\n",
    ":::{dropdown} Are artifacts aware of array-like data?\n",
    "\n",
    "Yes.\n",
    "\n",
    "You can make artifacts from paths referencing array-like objects:\n",
    "\n",
    "```python\n",
    "ln.Artifact(\"./my_anndata.h5ad\", key=\"my_anndata.h5ad\")\n",
    "ln.Artifact(\"./my_zarr_array/\", key=\"my_zarr_array\")\n",
    "```\n",
    "\n",
    "Or from in-memory objects:\n",
    "\n",
    "```python\n",
    "ln.Artifact.from_df(df, key=\"my_dataframe.parquet\")\n",
    "ln.Artifact.from_anndata(adata, key=\"my_anndata.h5ad\")\n",
    "```\n",
    "\n",
    "You can open large artifacts for slicing from the cloud or load small artifacts directly into memory.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like transforms, artifacts are versioned. Let's create a new version by revising the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# keep the dataframe with a typo around - we'll need it later\n",
    "df_typo = df.copy()\n",
    "\n",
    "# fix the \"IFNJ\" typo\n",
    "df[\"perturbation\"] = df[\"perturbation\"].cat.rename_categories({\"IFNJ\": \"IFNG\"})\n",
    "\n",
    "# create a new version\n",
    "artifact = ln.Artifact.from_df(df, key=\"my_datasets/rnaseq1.parquet\").save()\n",
    "\n",
    "# see all versions of an artifact\n",
    "artifact.versions.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Can I also create new versions independent of `key`?\n",
    "\n",
    "That works, too, you can use `revises`:\n",
    "\n",
    "```python\n",
    "artifact_v1 = ln.Artifact.from_df(df, description=\"Just a description\").save()\n",
    "# below revises artifact_v1\n",
    "artifact_v2 = ln.Artifact.from_df(df_updated, revises=artifact_v1).save()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The good thing about passing `revises: Artifact` is that you don't need to worry about coming up with naming conventions for paths.\n",
    "\n",
    "The good thing about versioning based on `key` is that it's how all data versioning tools are doing it.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Annotate an artifact with a {class}`~lamindb.ULabel` and a {class}`bionty.CellType`. The same works for any entity in any custom schema module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import bionty as bt\n",
    "\n",
    "# create & save a typed label\n",
    "experiment_type = ln.ULabel(name=\"Experiment\", is_type=True).save()\n",
    "candidate_marker_experiment = ln.ULabel(\n",
    "    name=\"Candidate marker experiment\", type=experiment_type\n",
    ").save()\n",
    "\n",
    "# label the artifact\n",
    "artifact.ulabels.add(candidate_marker_experiment)\n",
    "\n",
    "# repeat for a bionty entity\n",
    "cell_type = bt.CellType.from_source(name=\"effector T cell\").save()\n",
    "artifact.cell_types.add(cell_type)\n",
    "\n",
    "# describe the artifact\n",
    "artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For annotating datasets with parsed labels like the cell_mediums `DMSO` & `IFNG`, jump to \"Curate datasets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaminDB's central classes are registries that store records ({class}`~lamindb.core.Record` objects).\n",
    "\n",
    "The easiest way to see the latest records for a registry is to call the _class method_ {class}`~lamindb.core.Record.df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.ULabel.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A record and its registry share the same fields, which define the metadata you can query for. If you want to see them, look at the class or auto-complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query & search\n",
    "\n",
    "You can write arbitrary relational queries using the class methods {class}`~lamindb.core.Record.get` and {class}`~lamindb.core.Record.filter`.\n",
    "The syntax for it is Django's query syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single record (here the current notebook)\n",
    "transform = ln.Transform.get(key=\"introduction.ipynb\")\n",
    "\n",
    "# get a set of records by filtering on description\n",
    "ln.Artifact.filter(key__startswith=\"my_datasets/\").df()\n",
    "\n",
    "# query all artifacts ingested from a transform\n",
    "artifacts = ln.Artifact.filter(transform=transform).all()\n",
    "\n",
    "# query all artifacts ingested from a notebook with \"intro\" in the title and labeled \"Candidate marker experiment\"\n",
    "artifacts = ln.Artifact.filter(\n",
    "    transform__description__icontains=\"intro\", ulabels=candidate_marker_experiment\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class methods {class}`~lamindb.core.Record.search` and {class}`~lamindb.core.Record.lookup` help with approximate matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in a registry\n",
    "ln.Transform.search(\"intro\").df()\n",
    "\n",
    "# look up records with auto-complete\n",
    "ulabels = ln.ULabel.lookup()\n",
    "cell_types = bt.CellType.lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Show me a screenshot\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/lgRNHNtMxjU0y8nIagt7.png\" width=\"400px\">\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can annotate datasets by associated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# define the \"temperature\" & \"experiment\" features\n",
    "ln.Feature(name=\"temperature\", dtype=float).save()\n",
    "ln.Feature(\n",
    "    name=\"experiment\", dtype=ln.ULabel\n",
    ").save()  # categorical values are validated against the ULabel registry\n",
    "\n",
    "# annotate\n",
    "artifact.features.add_values(\n",
    "    {\"temperature\": 21.6, \"experiment\": \"Candidate marker experiment\"}\n",
    ")\n",
    "\n",
    "# describe the artifact\n",
    "artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query artifacts by features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Artifact.features.filter(experiment__contains=\"marker experiment\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to validate & annotate a dataset by the features they measure is via a `Curator`: jump to \"Curate datasets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand data lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand where a dataset comes from and what it's used for ([background](inv:docs#project-flow)).\n",
    "\n",
    "```python\n",
    "artifact.view_lineage()\n",
    "```\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/KQmzmmLOeBN0C8Ykitjn.svg\" width=\"800\">\n",
    "\n",
    ":::{dropdown} I just want to see the transforms.\n",
    "\n",
    "```python\n",
    "transform.view_lineage()\n",
    "```\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/b0geN1HDHXlORqMOOPay.svg\" width=\"400\">\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need a workflow manager to track data lineage (if you want to use one, see {doc}`docs:pipelines`). All you need is:\n",
    "\n",
    "```python\n",
    "import lamindb as ln\n",
    "\n",
    "ln.track()  # track your run, start tracking inputs & outputs\n",
    "\n",
    "# your code\n",
    "\n",
    "ln.finish()  # mark run as finished, save execution report, source code & environment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} On the hub.\n",
    "\n",
    "Below is how a single transform ([a notebook](https://lamin.ai/laminlabs/lamindata/transform/PtTXoc0RbOIq65cN)) with its run report looks on the hub.\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/RGXj5wcAf7EAc6J8dJfH.png\" width=\"900px\">\n",
    "\n",
    "```\n",
    "\n",
    "To create a new version of a notebook or script, run `lamin load` on the terminal, e.g.,\n",
    "\n",
    "```bash\n",
    "$ lamin load https://lamin.ai/laminlabs/lamindata/transform/13VINnFk89PE0004\n",
    "→ notebook is here: mcfarland_2020_preparation.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curate datasets\n",
    "\n",
    "You already saw how to ingest datasets without validation.\n",
    "This is often enough if you're prototyping or working with one-off studies.\n",
    "But if you want to create a big body of standardized data, you have to invest the time to curate your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a {class}`~lamindb.Schema` to curate a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# define valid labels\n",
    "perturbation_type = ln.ULabel(name=\"Perturbation\", is_type=True).save()\n",
    "ln.ULabel(name=\"DMSO\", type=perturbation_type).save()\n",
    "ln.ULabel(name=\"IFNG\", type=perturbation_type).save()\n",
    "\n",
    "# define the schema\n",
    "schema = ln.Schema(\n",
    "    name=\"My DataFrame schema\",\n",
    "    features=[\n",
    "        ln.Feature(name=\"ENSG00000153563\", dtype=int).save(),\n",
    "        ln.Feature(name=\"ENSG00000010610\", dtype=int).save(),\n",
    "        ln.Feature(name=\"ENSG00000170458\", dtype=int).save(),\n",
    "        ln.Feature(name=\"perturbation\", dtype=ln.ULabel).save(),\n",
    "    ],\n",
    ").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `Curator`, we can save an _annotated_ & _validated_ artifact with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "curator = ln.curators.DataFrameCurator(df, schema)\n",
    "\n",
    "# save curated artifact\n",
    "artifact = curator.save_artifact(key=\"my_curated_dataset.parquet\")  # calls .validate()\n",
    "\n",
    "# see the parsed annotations\n",
    "artifact.describe()\n",
    "\n",
    "# query for a ulabel that was parsed from the dataset\n",
    "ln.Artifact.get(ulabels__name=\"IFNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feed a dataset with an invalid dtype or typo, we'll get a `ValidationError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "curator = ln.curators.DataFrameCurator(df_typo, schema)\n",
    "\n",
    "# validate the dataset\n",
    "try:\n",
    "    curator.validate()\n",
    "except ln.errors.ValidationError as error:\n",
    "    print(str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage biological registries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic {class}`~lamindb.Feature` and {class}`~lamindb.ULabel` registries will get you pretty far.\n",
    "\n",
    "But let's now look at what you do can with a dedicated biological registry like {class}`~bionty.Gene`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every {py:mod}`bionty` registry is based on configurable public ontologies (>20 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "cell_types = bt.CellType.public()\n",
    "cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "cell_types.search(\"gamma-delta T cell\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an `AnnData` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define var schema\n",
    "var_schema = ln.Schema(\n",
    "    name=\"my_var_schema\",\n",
    "    itype=bt.Gene.ensembl_gene_id,\n",
    "    dtype=int,\n",
    ").save()\n",
    "\n",
    "obs_schema = ln.Schema(\n",
    "    name=\"my_obs_schema\",\n",
    "    features=[\n",
    "        ln.Feature(name=\"perturbation\", dtype=ln.ULabel).save(),\n",
    "    ],\n",
    ").save()\n",
    "\n",
    "# define composite schema\n",
    "anndata_schema = ln.Schema(\n",
    "    name=\"my_anndata_schema\",\n",
    "    otype=\"AnnData\",\n",
    "    components={\"obs\": obs_schema, \"var\": var_schema},\n",
    ").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate & annotate an `AnnData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import bionty as bt\n",
    "\n",
    "# store the dataset as an AnnData object to distinguish data from metadata\n",
    "adata = ad.AnnData(\n",
    "    df[[\"ENSG00000153563\", \"ENSG00000010610\", \"ENSG00000170458\"]],\n",
    "    obs=df[[\"perturbation\"]],\n",
    ")\n",
    "\n",
    "# save curated artifact\n",
    "curator = ln.curators.AnnDataCurator(adata, anndata_schema)\n",
    "artifact = curator.save_artifact(description=\"my RNA-seq\")\n",
    "artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for typed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# get a lookup object for human genes\n",
    "genes = bt.Gene.filter(organism__name=\"human\").lookup()\n",
    "# query for all feature sets that contain CD8A\n",
    "feature_sets = ln.FeatureSet.filter(genes=genes.cd8a).all()\n",
    "# write the query\n",
    "ln.Artifact.filter(feature_sets__in=feature_sets).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update ontologies, e.g., create a cell type record and add a new cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# create an ontology-coupled cell type record and save it\n",
    "neuron = bt.CellType.from_source(name=\"neuron\").save()\n",
    "\n",
    "# create a record to track a new cell state\n",
    "new_cell_state = bt.CellType(\n",
    "    name=\"my neuron cell state\", description=\"explains X\"\n",
    ").save()\n",
    "\n",
    "# express that it's a neuron state\n",
    "new_cell_state.parents.add(neuron)\n",
    "\n",
    "# view ontological hierarchy\n",
    "new_cell_state.view_parents(distance=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you integrate new datasets with your existing datasets? Leverage {class}`~lamindb.Collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# a new dataset\n",
    "df2 = ln.core.datasets.small_dataset2(otype=\"DataFrame\")\n",
    "adata = ad.AnnData(\n",
    "    df2[[\"ENSG00000153563\", \"ENSG00000010610\", \"ENSG00000004468\"]],\n",
    "    obs=df2[[\"perturbation\"]],\n",
    ")\n",
    "curator = ln.curators.AnnDataCurator(adata, anndata_schema)\n",
    "artifact2 = curator.save_artifact(key=\"my_datasets/my_rnaseq2.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a collection using {class}`~lamindb.Collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "collection = ln.Collection([artifact, artifact2], key=\"my-RNA-seq-collection\").save()\n",
    "collection.describe()\n",
    "collection.view_lineage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# if it's small enough, you can load the entire collection into memory as if it was one\n",
    "collection.load()\n",
    "\n",
    "# typically, it's too big, hence, open it for streaming (if the backend allows it)\n",
    "# collection.open()\n",
    "\n",
    "# or iterate over its artifacts\n",
    "collection.artifacts.all()\n",
    "\n",
    "# or look at a DataFrame listing the artifacts\n",
    "collection.artifacts.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly train models on collections of `AnnData`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# to train models, batch iterate through the collection as if it was one array\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "dataset = collection.mapped(obs_keys=[\"cell_medium\"])\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=dataset.get_label_weights(\"cell_medium\"), num_samples=len(dataset)\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=2, sampler=sampler)\n",
    "for batch in data_loader:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this [blog post](https://lamin.ai/blog/arrayloader-benchmarks) for more on training models on sharded datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "### World model\n",
    "\n",
    "1. Teams need to have enough freedom to initiate work independently but enough structure to easily integrate datasets later on\n",
    "2. Batched datasets ({class}`~lamindb.Artifact`) from physical instruments are transformed ({class}`~lamindb.Transform`) into useful representations\n",
    "3. Learning needs features ({class}`~lamindb.Feature`, {class}`~bionty.CellMarker`, ...) and labels ({class}`~lamindb.ULabel`, {class}`~bionty.CellLine`, ...)\n",
    "4. Insights connect dataset representations with experimental metadata and knowledge (ontologies)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "LaminDB is a distributed system like git that can be run or hosted anywhere. As infrastructure, you merely need a database (SQLite/Postgres) and a storage location (file system, S3, GCP, HuggingFace, ...).\n",
    "\n",
    "You can easily create your new local instance:\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Shell\n",
    "```bash\n",
    "lamin init --storage ./my-data-folder\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Python\n",
    "```python\n",
    "import lamindb as ln\n",
    "ln.setup.init(storage=\"./my-data-folder\")\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "Or you can let collaborators connect to a cloud-hosted instance:\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Shell\n",
    "```bash\n",
    "lamin connect account-handle/instance-name\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Python\n",
    "```python\n",
    "import lamindb as ln\n",
    "ln.connect(\"account-handle/instance-name\")\n",
    "```\n",
    ":::\n",
    ":::{tab-item} R\n",
    "```R\n",
    "library(laminr)\n",
    "ln <- connect(\"account-handle/instance-name\")\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "For learning more about how to create & host LaminDB instances on distributed infrastructure, see {doc}`setup`. LaminDB instances work standalone but can optionally be managed by LaminHub. For an architecture diagram of LaminHub, [reach out](https://lamin.ai/contact)!\n",
    "\n",
    "\n",
    "### Database schema & API\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/XoTQFCmmj2uU4d2xyj9u.png\" width=\"350px\" style=\"background: transparent\" align=\"right\">\n",
    "\n",
    "LaminDB provides a SQL schema for common metadata entities: {class}`~lamindb.Artifact`, {class}`~lamindb.Collection`, {class}`~lamindb.Transform`, {class}`~lamindb.Feature`, {class}`~lamindb.ULabel` etc. - see the [API reference](/api) or the [source code](https://github.com/laminlabs/lnschema-core/blob/main/lnschema_core/models.py).\n",
    "\n",
    "The core metadata schema is extendable through modules (see green vs. red entities in **graphic**), e.g., with basic biological ({class}`~bionty.Gene`, {class}`~bionty.Protein`, {class}`~bionty.CellLine`, etc.) & operational entities (`Biosample`, `Techsample`, `Treatment`, etc.).\n",
    "\n",
    "```{dropdown} What is the metadata schema language?\n",
    "\n",
    "Data models are defined in Python using the Django ORM. Django translates them to SQL tables.\n",
    "[Django](https://github.com/django/django) is one of the most-used & highly-starred projects on GitHub (~1M dependents, ~73k stars) and has been robustly maintained for 15 years.\n",
    "\n",
    "```\n",
    "\n",
    "On top of the metadata schema, LaminDB is a Python API that models datasets as artifacts, abstracts over storage & database access, data transformations, and (biological) ontologies.\n",
    "\n",
    "Note that the schemas of datasets (e.g., `.parquet` files, `.h5ad` arrays, etc.) are modeled through the `Feature` registry and do not require migrations to be updated.\n",
    "\n",
    "### Custom registries\n",
    "\n",
    "LaminDB can be extended with registry modules building on the [Django](https://github.com/django/django) ecosystem. Examples are:\n",
    "\n",
    "- [bionty](./bionty): Registries for basic biological entities, coupled to public ontologies.\n",
    "- [wetlab](https://github.com/laminlabs/wetlab): Registries for samples, treatments, etc.\n",
    "\n",
    "If you'd like to create your own module:\n",
    "\n",
    "1. Create a git repository with registries similar to [wetlab](https://github.com/laminlabs/wetlab)\n",
    "2. Create & deploy migrations via `lamin migrate create` and `lamin migrate deploy`\n",
    "\n",
    "### Repositories\n",
    "\n",
    "LaminDB and its plugins consist in open-source Python libraries & publicly hosted metadata assets:\n",
    "\n",
    "- [lamindb](https://github.com/laminlabs/lamindb): Core package.\n",
    "- [bionty](https://github.com/laminlabs/bionty): Registries for basic biological entities, coupled to public ontologies.\n",
    "- [wetlab](https://github.com/laminlabs/wetlab): Registries for samples, treatments, etc.\n",
    "- [usecases](https://github.com/laminlabs/lamin-usecases): Use cases as visible on the docs.\n",
    "\n",
    "All immediate dependencies are available as git submodules [here](https://github.com/laminlabs/lamindb/tree/main/sub), for instance,\n",
    "\n",
    "- [lamindb-setup](https://github.com/laminlabs/lamindb-setup): Setup & configure LaminDB.\n",
    "- [lamin-cli](https://github.com/laminlabs/lamin-cli): CLI for `lamindb` and `lamindb-setup`.\n",
    "\n",
    "For a comprehensive list of open-sourced software, browse our [GitHub account](https://github.com/laminlabs).\n",
    "\n",
    "- [lamin-utils](https://github.com/laminlabs/lamin-utils): Generic utilities, e.g., a logger.\n",
    "- [readfcs](https://github.com/laminlabs/readfcs): FCS artifact reader.\n",
    "- [nbproject](https://github.com/laminlabs/readfcs): Light-weight Jupyter notebook tracker.\n",
    "- [bionty-assets](https://github.com/laminlabs/bionty-assets): Assets for public biological ontologies.\n",
    "\n",
    "LaminHub is not open-sourced.\n",
    "\n",
    "### Influences\n",
    "\n",
    "LaminDB was influenced by many other projects, see {doc}`docs:influences`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
