LaminDB is an open-source data framework to enable learning at scale in computational biology.
It lets you track data transformations, curate datasets, manage metadata, and query a built-in database for biological entities & data structures.

:::{dropdown} Why?

Reproducing analytical results or understanding how a dataset or model was created can be a pain.
Leave alone training models on historical data, orthogonal assays, or datasets generated by other teams.

Biological datasets are typically managed with versioned storage systems (file systems, object storage, git, dvc), {term}`UI`-focused community or SaaS platforms, structureless data lakes, rigid data warehouses (SQL, monolithic arrays), and data lakehouses for tabular data.

LaminDB goes beyond these systems with a lakehouse that models biological datasets beyond tables with enough structure to enable queries and enough freedom to keep the pace of R&D high.

For data structures like `DataFrame`, `AnnData`, `.zarr`, `.tiledbsoma`, etc., LaminDB tracks and provides the rich context that collaborative biological research requires:

- data lineage: data sources and transformations; scientists and machine learning models
- domain knowledge and experimental metadata: the features and labels derived from domain entities

In this [blog post](https://lamin.ai/blog/problems), we discuss a breadth of data management problems of the field.

:::

:::{dropdown} LaminDB specs

```{include} includes/features-lamindb.md

```

:::

LaminHub is a data collaboration hub built on LaminDB similar to how GitHub is built on git.

:::{dropdown} LaminHub overview

```{include} includes/features-laminhub.md

```

:::

## Quickstart

::::{tab-set}
:::{tab-item} Python
:sync: python

Install the `lamindb` Python package.

```shell
pip install 'lamindb[jupyter,bionty]'  # support notebooks & biological ontologies
```

Connect to a LaminDB instance.

```shell
lamin connect account/instance  # <-- replace with your instance
```

Access an input dataset and save an output dataset.

```python
import lamindb as ln

ln.track()  # track a run of your notebook or script
artifact = ln.Artifact.get("3TNCsZZcnIBv2WGb0001")  # get an artifact by uid
filepath = artifact.cache()  # cache the artifact on disk

# do your work

ln.Artifact("./my_dataset.csv", key="my_results/my_dataset.csv").save()  # save a file
ln.finish()  # mark the run as finished & save a report for the current notebook/script
```

:::
:::{tab-item} R
:sync: r

Install the `laminr` R and the `lamindb` Python packages.

```R
install.packages("laminr", dependencies = TRUE)  # install the laminr package from CRAN
laminr::install_lamindb(extra_packages = c("bionty"))  # install lamindb & bionty for use via reticulate
```

Connect to a LaminDB instance.

```R
laminr::lamin_connect("<account>/<instance>")  # <-- replace with your instance
```

Access an input dataset and save an output dataset.

```R
library(laminr)
ln <- import_module("lamindb")  # instantiate the central `ln` object of the API

ln$track()  # track a run of your notebook or script
artifact <- ln$Artifact$get("3TNCsZZcnIBv2WGb0001")  # get an artifact by uid
filepath <- artifact$cache()  # cache the artifact on disk

# do your work

ln$Artifact("./my_dataset.csv", key="my_results/my_dataset.csv").save()  # save a file
ln$finish()  # mark the run finished
```

If you did _not_ use RStudio's notebook mode, save your notebook to see it on LaminHub.

```shell
lamin save my-analysis.Rmd  #  save an html report for a `.qmd` or `.Rmd` file
```

For more, see the [R docs](https://laminr.lamin.ai/).

:::
::::
