LaminDB is an open-source data framework for better computational biology.
It lets you track data transformations, curate datasets, manage metadata, and query a built-in database for common biological data types.

:::{dropdown} Why?

<img src="https://lamin-site-assets.s3.amazonaws.com/.lamindb/BunYmHkyFLITlM5MYQck.svg" width="350px" style="background: transparent" align="right">

Reproducing analytical results or understanding how a dataset or model was created can be a pain.
Leave alone training models on historical data, orthogonal assays, or datasets generated by other teams.

Biological datasets are typically managed with versioned storage systems (file systems, object storage, git, dvc), {term}`UI`-focused community or SaaS platforms, structureless data lakes, rigid data warehouses (SQL, monolithic arrays), and data lakehouses for tabular data.

LaminDB goes beyond these systems with a lakehouse that models biological datasets beyond tables with enough structure to enable queries and enough freedom to keep the pace of R&D high.

For data structures like `DataFrame`, `AnnData`, `.zarr`, `.tiledbsoma`, etc., LaminDB tracks and provides the rich context that collaborative biological research requires:

- data lineage: data sources and transformations; scientists and machine learning models
- domain knowledge and experimental metadata: the features and labels derived from domain entities

In this [blog post](https://lamin.ai/blog/problems), we discuss a breadth of data management problems of the field.

:::

:::{dropdown} LaminDB specs

```{include} includes/features-lamindb.md

```

:::

LaminHub is a data collaboration hub built on LaminDB similar to how GitHub is built on git.

:::{dropdown} LaminHub overview

```{include} includes/features-laminhub.md

```

:::

## Quickstart

Install the `lamindb` Python package.

```shell
pip install 'lamindb[jupyter,bionty]'  # support notebooks & biological ontologies
```

Connect to a LaminDB instance.

```shell
lamin connect account/instance  # <-- replace with your instance
```

Access an input dataset and save an output dataset.

::::{tab-set}
:::{tab-item} Python

```python
import lamindb as ln

ln.track()  # track a run of your notebook or script
artifact = ln.Artifact.get("3TNCsZZcnIBv2WGb0001")  # get an artifact by uid
filepath = artifact.cache()  # cache the artifact on disk

# do your work

ln.Artifact("./my_dataset.csv", key="my_results/my_dataset.csv").save()  # save a file
ln.finish()  # mark the run as finished & save a report for the current notebook/script
```

:::
:::{tab-item} R

```R
install.packages("laminr", dependencies = TRUE)  # install the laminr package from CRAN
laminr::install_lamindb()  # install lamindb for usage via reticulate
laminr::lamin_connect("<account>/<instance>")  # <-- replace with your instance
library(laminr)

ln <- import_module("lamindb")
ln$track()  # track a run of your notebook or script
artifact <- ln$Artifact$get("3TNCsZZcnIBv2WGb0001")  # get an artifact by uid
filepath <- artifact$cache()  # cache the artifact on disk

# do your work

ln$Artifact("./my_dataset.csv", key="my_results/my_dataset.csv").save()  # save a folder
ln$finish()  # mark the run finished
```

Depending on whether you ran RStudio's notebook mode, you may need to save an html export for `.qmd` or `.Rmd` file via the command-line.

```shell
lamin save my-analysis.Rmd
```

For more, see the [R docs](https://laminr.lamin.ai/).

:::
::::
