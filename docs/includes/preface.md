LaminDB is an open-source data framework to enable learning at scale in computational biology.
It lets you track data transformations, curate datasets, manage metadata, and query a built-in database for biological entities & data structures.

:::{dropdown} Why?

Reproducing analytical results or understanding how a dataset or model was created can be a pain.
Leave alone training models on historical data, orthogonal assays, or datasets generated by other teams.

Biological datasets are typically managed with versioned storage systems (file systems, object storage, git, dvc), {term}`UI`-focused community or SaaS platforms, structureless data lakes, rigid data warehouses (SQL, monolithic arrays), and data lakehouses for tabular data.

LaminDB goes beyond these systems with a lakehouse that models biological datasets beyond tables with enough structure to enable queries and enough freedom to keep the pace of R&D high.

For data structures like `DataFrame`, `AnnData`, `.zarr`, `.tiledbsoma`, etc., LaminDB tracks and provides the rich context that collaborative biological research requires:

- data lineage: data sources and transformations; scientists and machine learning models
- domain knowledge and experimental metadata: the features and labels derived from domain entities

In this [blog post](https://lamin.ai/blog/problems), we discuss a breadth of data management problems of the field.

:::

:::{dropdown} LaminDB specs

```{include} includes/features-lamindb.md

```

:::

LaminHub is a data collaboration hub built on LaminDB similar to how GitHub is built on git.

:::{dropdown} LaminHub overview

```{include} includes/features-laminhub.md

```

:::

## Quickstart

::::{tab-set}
:::{tab-item} Python
:sync: python

For setup, install the `lamindb` Python package and connect to a LaminDB instance.

```shell
pip install 'lamindb[jupyter,bionty]'  # support notebooks & biological ontologies
lamin login  # <-- you can skip this for public, local & self-hosted instances
lamin connect account/instance  # <-- replace with your instance
```

In your Python session, you access an input dataset and save an output dataset.

```python
import lamindb as ln

ln.track()  # track a run of your notebook or script
artifact = ln.Artifact.get("3TNCsZZcnIBv2WGb0001")  # get an artifact by uid
filepath = artifact.cache()  # cache the artifact on disk

# do your work

ln.Artifact("./my_dataset.csv", key="my_results/my_dataset.csv").save()  # save a file
ln.finish()  # mark the run as finished & save a report for the current notebook/script
```

:::
:::{tab-item} R
:sync: r

For setup, install the `laminr` and `lamindb` packages and connect to a LaminDB instance.

```R
install.packages("laminr", dependencies = TRUE)  # install the laminr package from CRAN
laminr::install_lamindb(extra_packages = c("bionty"))  # install lamindb & bionty for use via reticulate
laminr::lamin_login()  # <-- you can skip this for public, local, and self-hosted instances
laminr::lamin_connect("<account>/<instance>")  # <-- replace with your instance
```

In your R session, you access an input dataset and save an output dataset.

```{eval-rst}
.. literalinclude:: includes/r-quickstart.R
   :language: R
```

If you did _not_ use RStudio's notebook mode, create an html export and then run the following.

```R
laminr::lamin_save("my-analyis.Rmd")  #  save source code and html report for a `.qmd` or `.Rmd` file
```

:::
::::
